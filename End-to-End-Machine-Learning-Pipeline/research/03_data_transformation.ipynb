{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7DZ-BJJ6ixU"
      },
      "source": [
        "* project ç›®éŒ„ç‚º `End-to-End-Machine-Learning-Pipeline`ï¼Œä»¥ä¸‹è·¯å¾‘å‡ä»¥æ­¤ç‚º æ ¹ç›®éŒ„"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeDBuBAD6ixZ"
      },
      "source": [
        "* `src/mlProject/constants/__init__.py` ä¸‹å®šç¾©äº†\n",
        "\n",
        "\t```python\n",
        "\tCONFIG_FILE_PATH\n",
        "\tPARAMS_FILE_PATH\n",
        "\tSCHEMA_FILE_PATH\n",
        "\t```\n",
        "\n",
        "* `src/mlProject/constants/__init__.py` è¦å…ˆå®Œæˆ (é‡å° 5. Update the configuration manager )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4muyi_o6ixa"
      },
      "source": [
        "# è‡ªè¡Œå¯¦ä½œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Uxp6HpB6ixa"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/henrykohl/MLOps-Foundation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QWlcEReE6ixc",
        "outputId": "fd6cb3b6-ce43-45b7-d849-cc9d64fc51b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ğŸ“¦ Installing...\n",
            "ğŸ“Œ Adjusting configuration...\n",
            "ğŸ©¹ Patching environment...\n",
            "â² Done in 0:00:11\n",
            "ğŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install() # expect a kernel restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpiLK8S66ixd"
      },
      "outputs": [],
      "source": [
        "!conda create -n mlproj python=3.8 -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JGnRzyij6ixc",
        "outputId": "cfd74ebe-e0df-4694-f8be-a5a3e3709206",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MLOps-Foundation/End-to-End-Machine-Learning-Pipeline\n"
          ]
        }
      ],
      "source": [
        "%cd MLOps-Foundation/End-to-End-Machine-Learning-Pipeline/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FTXoR4jb6ixd",
        "outputId": "ea64989b-cad0-4ad1-e566-55fe8f957c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/MLOps-Foundation/End-to-End-Machine-Learning-Pipeline'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRFwvi7R6ixe"
      },
      "outputs": [],
      "source": [
        "!source activate mlproj; pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* colab æ“ä½œæ™‚ï¼Œä¾åº\n",
        "  - å°‡ `01_data_ingestion.ipynb` ä¸­ï¼Œè‡ªè¡Œå¯¦ä½œçš„åŸ·è¡Œéƒ¨åˆ†è¤‡è£½éä¾†ï¼ŒåŸ·è¡Œä¸€æ¬¡ã€‚\n",
        "  - å°‡ `02_data_validation.ipynb` ä¸­ï¼Œè‡ªè¡Œå¯¦ä½œçš„åŸ·è¡Œéƒ¨åˆ†è¤‡è£½éä¾†ï¼ŒåŸ·è¡Œä¸€æ¬¡ã€‚"
      ],
      "metadata": {
        "id": "-OKAEzpTQIL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z5TuOZXO6ixe",
        "outputId": "82f43456-2101-464a-fdf2-685ab78125a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-12-16 07:24:01,048: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
            "[2025-12-16 07:24:01,049: INFO: common: yaml file: params.yaml loaded successfully]\n",
            "[2025-12-16 07:24:01,051: INFO: common: yaml file: schema.yaml loaded successfully]\n",
            "[2025-12-16 07:24:01,051: INFO: common: created directory at: artifacts]\n",
            "[2025-12-16 07:24:01,052: INFO: common: created directory at: artifacts/data_transformation]\n",
            "[2025-12-16 07:24:01,083: INFO: <stdin>: Splited data into training and test sets]\n",
            "[2025-12-16 07:24:01,083: INFO: <stdin>: (1199, 12)]\n",
            "[2025-12-16 07:24:01,083: INFO: <stdin>: (400, 12)]\n",
            "(1199, 12)\n",
            "(400, 12)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "source activate mlproj\n",
        "\n",
        "python\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class DataTransformationConfig:\n",
        "    root_dir: Path           ## å¯¦ä¾‹åŒ–æ™‚ï¼Œå„²å­˜çš„é¡å‹æ˜¯ str\n",
        "    data_path: Path\n",
        "\n",
        "from mlProject.constants import *\n",
        "from mlProject.utils.common import read_yaml, create_directories\n",
        "\n",
        "class ConfigurationManager:\n",
        "    def __init__(\n",
        "        self,\n",
        "        config_filepath = CONFIG_FILE_PATH,   ## è¼¸å‡º: PosixPath(\"config/config.yaml\")\n",
        "        params_filepath = PARAMS_FILE_PATH,   ## è¼¸å‡º: PosixPath(\"params.yaml\")\n",
        "        schema_filepath = SCHEMA_FILE_PATH):  ## è¼¸å‡º: PosixPath(\"schema.yaml\")\n",
        "\n",
        "        self.config = read_yaml(config_filepath) ## è¼¸å‡º: ConfigBox({...}); config.artifacts_root æ˜¯ str\n",
        "        self.params = read_yaml(params_filepath) ## è¼¸å‡º: ConfigBox({...})\n",
        "        self.schema = read_yaml(schema_filepath) ## è¼¸å‡º: ConfigBox({...})\n",
        "\n",
        "        create_directories([self.config.artifacts_root]) ## å»ºç«‹ç›®éŒ„ artifacts\n",
        "\n",
        "\n",
        "\n",
        "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
        "        config = self.config.data_transformation ## è¼¸å‡º: ConfigBox({...});\n",
        "\n",
        "        create_directories([config.root_dir]) ## å»ºç›®éŒ„ artifacts/data_ingestion; config.root_dir æ˜¯ str\n",
        "\n",
        "        data_transformation_config = DataTransformationConfig(\n",
        "            root_dir=config.root_dir,         ## config.root_dir æ˜¯ str\n",
        "            data_path=config.data_path,       ## config.data_path æ˜¯ str\n",
        "        )\n",
        "\n",
        "        return data_transformation_config\n",
        "\n",
        "import os\n",
        "from mlProject import logger\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "class DataTransformation:\n",
        "    def __init__(self, config: DataTransformationConfig):\n",
        "        self.config = config\n",
        "\n",
        "\n",
        "    ## Note: You can add different data transformation techniques such as Scaler, PCA and all\n",
        "    #You can perform all kinds of EDA in ML cycle here before passing this data to the model\n",
        "\n",
        "    # I am only adding train_test_spliting cz this data is already cleaned up\n",
        "\n",
        "\n",
        "    def train_test_spliting(self):\n",
        "        data = pd.read_csv(self.config.data_path) ## æª”æ¡ˆ artifacts/data_ingestion/winequality-red.csv è·¯å¾‘\n",
        "\n",
        "        # Split the data into training and test sets. (0.75, 0.25) split.\n",
        "        train, test = train_test_split(data)\n",
        "\t\t\t\t## 'train' å­˜æª”è·¯å¾‘ artifacts/data_transformation/train.csv\n",
        "        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False)\n",
        "\t\t\t\t## 'test' å­˜æª”è·¯å¾‘ artifacts/data_transformation/test.csv\n",
        "        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\n",
        "\n",
        "        logger.info(\"Splited data into training and test sets\")\n",
        "        logger.info(train.shape)\n",
        "        logger.info(test.shape)\n",
        "\n",
        "        print(train.shape)\n",
        "        print(test.shape)\n",
        "\n",
        "\n",
        "try:\n",
        "    config = ConfigurationManager()                                            ## ä¾‹åŒ–configurationï¼Œå»ºç«‹ä¸»ç›®éŒ„\n",
        "    data_transformation_config = config.get_data_transformation_config()       ## åŸ·è¡Œconfigurationï¼Œå»ºç«‹æ¬¡ç›®éŒ„ï¼Œä¾‹åŒ–entit\n",
        "    data_transformation = DataTransformation(config=data_transformation_config)## ä¾‹åŒ– component\n",
        "    data_transformation.train_test_spliting()                                  ## åŸ·è¡Œ componentï¼Œé€²è¡Œè³‡æ–™åˆ†å‰²\n",
        "except Exception as e:\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0PJgdK96ixe"
      },
      "source": [
        "# Lecture Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqTMVUDJ6ixe",
        "outputId": "b1a7a5db-6155-4973-c80f-0b3ffbc61c3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\Bappy\\\\Live Sessions\\\\Euron\\\\MLOPs Masters Batch\\\\End-to-End-Machine-Learning-Pipeline'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(\"../\")\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3bIjsPA6ixf"
      },
      "source": [
        "## 4. Update the entity\n",
        "* project å°æ‡‰ `src/mlProject/entity/config_entity.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMT5VSlx6ixg"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class DataTransformationConfig:\n",
        "    root_dir: Path           ## å¯¦ä¾‹åŒ–æ™‚ï¼Œå„²å­˜çš„é¡å‹æ˜¯ str\n",
        "    data_path: Path          ## å¯¦ä¾‹åŒ–æ™‚ï¼Œå„²å­˜çš„é¡å‹æ˜¯ str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGdXmXpJ6ixg"
      },
      "source": [
        "## 5. Update the configuration manager\n",
        "* project å°æ‡‰ `src/mlProject/config/configuration.py`ç”¨åˆ°\n",
        "\t- '4. entity'ï¼š `src/mlProject/entity/config_entity.py` -- è¼¸å‡º DataTransformationConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wNtpgpL6ixg"
      },
      "outputs": [],
      "source": [
        "from mlProject.constants import *\n",
        "from mlProject.utils.common import read_yaml, create_directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4623Q4L6ixg"
      },
      "outputs": [],
      "source": [
        "class ConfigurationManager:\n",
        "    def __init__(\n",
        "        self,\n",
        "        config_filepath = CONFIG_FILE_PATH,   ## è¼¸å‡º: PosixPath(\"config/config.yaml\")\n",
        "        params_filepath = PARAMS_FILE_PATH,   ## è¼¸å‡º: PosixPath(\"params.yaml\")\n",
        "        schema_filepath = SCHEMA_FILE_PATH):  ## è¼¸å‡º: PosixPath(\"schema.yaml\")\n",
        "\n",
        "        self.config = read_yaml(config_filepath) ## è¼¸å‡º: ConfigBox({...}); config.artifacts_root æ˜¯ str\n",
        "        self.params = read_yaml(params_filepath) ## è¼¸å‡º: ConfigBox({...})\n",
        "        self.schema = read_yaml(schema_filepath) ## è¼¸å‡º: ConfigBox({...})\n",
        "\n",
        "        create_directories([self.config.artifacts_root]) ## å»ºç«‹ç›®éŒ„ artifacts\n",
        "\n",
        "\n",
        "\n",
        "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
        "        config = self.config.data_transformation ## è¼¸å‡º: ConfigBox({...});\n",
        "\n",
        "        create_directories([config.root_dir]) ## å»ºç›®éŒ„ artifacts/data_ingestion; config.root_dir æ˜¯ str\n",
        "\n",
        "        data_transformation_config = DataTransformationConfig(\n",
        "            root_dir=config.root_dir,         ## config.root_dir æ˜¯ str\n",
        "            data_path=config.data_path,       ## config.data_path æ˜¯ str\n",
        "        )\n",
        "\n",
        "        return data_transformation_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnJna9LC6ixg"
      },
      "source": [
        "## 6. Update the components\n",
        "* project å°æ‡‰ `src/mlProject/components/data_transformation.py` ç”¨åˆ°\n",
        "\t- '4. entity'ï¼š `src/mlProject/entity/config_entity.py` -- è¼¸å…¥ DataTransformationConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7wVnJ1z6ixh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from mlProject import logger\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpwDOL_P6ixh"
      },
      "outputs": [],
      "source": [
        "class DataTransformation:\n",
        "    def __init__(self, config: DataTransformationConfig):\n",
        "        self.config = config\n",
        "\n",
        "\n",
        "    ## Note: You can add different data transformation techniques such as Scaler, PCA and all\n",
        "    #You can perform all kinds of EDA in ML cycle here before passing this data to the model\n",
        "\n",
        "    # I am only adding train_test_spliting cz this data is already cleaned up\n",
        "\n",
        "\n",
        "    def train_test_spliting(self):\n",
        "        data = pd.read_csv(self.config.data_path) ## æª”æ¡ˆ artifacts/data_ingestion/winequality-red.csv è·¯å¾‘\n",
        "\n",
        "        # Split the data into training and test sets. (0.75, 0.25) split.\n",
        "        train, test = train_test_split(data)\n",
        "\t\t\t\t## 'train' å­˜æª”è·¯å¾‘ artifacts/data_transformation/train.csv\n",
        "        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False)\n",
        "\t\t\t\t## 'test' å­˜æª”è·¯å¾‘ artifacts/data_transformation/test.csv\n",
        "        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\n",
        "\n",
        "        logger.info(\"Splited data into training and test sets\")\n",
        "        logger.info(train.shape)\n",
        "        logger.info(test.shape)\n",
        "\n",
        "        print(train.shape)\n",
        "        print(test.shape)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KowpcGs36ixh"
      },
      "source": [
        "## 7. Update the pipeline\n",
        "* project å°æ‡‰ `src/mlProject/pipeline/stage_03_data_transformation.py` éœ€èª¿ç”¨\n",
        "\t- '5. configuration': `src/mlProject/config/configuration.py` -- è¼¸å‡º DataTransformationConfig\n",
        "\t- '6. components': `src/mlProject/components/data_ingestion.py` -- è¼¸å…¥ DataTransformationConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWUeiUss6ixh",
        "outputId": "d1ac28b7-8b05-4cbb-feb3-faae902bc2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-01-12 12:51:51,319: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
            "[2025-01-12 12:51:51,320: INFO: common: yaml file: params.yaml loaded successfully]\n",
            "[2025-01-12 12:51:51,321: INFO: common: yaml file: schema.yaml loaded successfully]\n",
            "[2025-01-12 12:51:51,322: INFO: common: created directory at: artifacts]\n",
            "[2025-01-12 12:51:51,323: INFO: common: created directory at: artifacts/data_transformation]\n",
            "[2025-01-12 12:51:51,335: INFO: 741865018: Splited data into training and test sets]\n",
            "[2025-01-12 12:51:51,335: INFO: 741865018: (1199, 12)]\n",
            "[2025-01-12 12:51:51,336: INFO: 741865018: (400, 12)]\n",
            "(1199, 12)\n",
            "(400, 12)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    config = ConfigurationManager()                                            ## ä¾‹åŒ–configurationï¼Œå»ºç«‹ä¸»ç›®éŒ„\n",
        "    data_transformation_config = config.get_data_transformation_config()       ## åŸ·è¡Œconfigurationï¼Œå»ºç«‹æ¬¡ç›®éŒ„ï¼Œä¾‹åŒ–entit\n",
        "    data_transformation = DataTransformation(config=data_transformation_config)## ä¾‹åŒ– component\n",
        "    data_transformation.train_test_spliting()                                  ## åŸ·è¡Œ componentï¼Œé€²è¡Œè³‡æ–™åˆ†å‰²\n",
        "except Exception as e:\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1u_PEcM6ixh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlproj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}